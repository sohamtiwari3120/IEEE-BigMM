{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0brZnNWfhBiH"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pytorch_pretrained_bert.modeling import BertModel\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VtJttWabwJR7"
   },
   "source": [
    "# **Image Handling Resnet-152**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i1MKtV0mkk46"
   },
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        model = torchvision.models.resnet152(pretrained=True)\n",
    "        modules = list(model.children())[:-2]\n",
    "        # we are removing the last adaptive average pooling layer and the \n",
    "        # the classification layer\n",
    "        self.model = nn.Sequential(*modules)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = (self.model(x))\n",
    "        # print('Model output', out.size())\n",
    "\n",
    "        out = nn.AdaptiveAvgPool2d((7, 1))(out)#specifying the H and W of the image\n",
    "        # to be obtained after pooling\n",
    "        # print('Pooling output', out.size())\n",
    "\n",
    "        out = torch.flatten(out, start_dim=2)\n",
    "        # print('Flattening output', out.size())\n",
    "\n",
    "        out = out.transpose(1, 2).contiguous()\n",
    "        # print('Transpose output', out.size())\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B7nRPX6mlhnA"
   },
   "source": [
    "\n",
    "### Important Note:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6k-f07rTwoWn"
   },
   "source": [
    "Understanding the process of obtaining N(in this case 7) 2048 dimensional image embeddings\n",
    "<br>\n",
    "<br>\n",
    "Below is the example of a sample image\n",
    "```\n",
    "img_enc = ImageEncoder()\n",
    "img = torch.randn(1, 3, 224, 224)\n",
    "img\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "tensor([[[[ 0.4623, -0.0570,  0.1685,  ..., -0.6377, -0.4702,  0.8996],\n",
    "          [ 0.5874,  0.1590, -0.2373,  ..., -1.7897, -0.3391, -1.0945],\n",
    "          [ 0.6259,  1.3741,  0.6457,  ..., -0.3259,  0.2340,  0.5563],\n",
    "          ...,\n",
    "          [-0.3431,  0.8013, -1.1648,  ...,  0.3589, -1.0933,  0.0880],\n",
    "          [ 0.3228, -2.2501,  1.8554,  ...,  0.6990,  1.2223, -0.6696],\n",
    "          [ 0.0949,  0.3022, -1.7768,  ...,  0.5936,  1.3039,  1.4402]],\n",
    "\n",
    "         [[-0.7338,  0.3525, -0.0956,  ..., -0.5781, -0.8532, -0.9768],\n",
    "          [ 0.3267, -0.4692,  0.2099,  ...,  0.8854, -0.0515, -0.9874],\n",
    "          [ 2.0738, -0.5577,  0.3773,  ...,  0.9743, -2.0519,  0.0128],\n",
    "          ...,\n",
    "          [-0.1382, -0.8803,  0.6664,  ..., -0.3854, -1.2113,  1.0680],\n",
    "          [-0.8094,  0.6352, -0.1113,  ..., -2.2602,  0.3099,  0.2487],\n",
    "          [-0.3672,  1.2410,  0.0260,  ..., -0.0627,  0.2084, -0.2197]],\n",
    "\n",
    "         [[ 0.6515, -0.2968, -0.1592,  ..., -0.0610,  0.3312, -0.9807],\n",
    "          [-1.9452, -1.1792, -0.3001,  ...,  0.5704,  1.4844, -1.4242],\n",
    "          [ 0.1115, -0.1929,  0.0363,  ...,  0.8737,  0.2437,  0.4418],\n",
    "          ...,\n",
    "          [ 1.6531,  0.0160, -0.6031,  ...,  0.8056, -0.5860, -0.2903],\n",
    "          [-0.1911, -1.4188, -0.2629,  ..., -1.3827, -0.7149, -2.4575],\n",
    "          [-1.5174, -1.5290, -0.3920,  ...,  1.0713,  0.4248, -0.2714]]]])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E9uWEL1GugDz"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "```\n",
    "img.size()\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "torch.Size([1, 3, 224, 224])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hB-g3adLnBfd"
   },
   "source": [
    "**Note:**<br>\n",
    "Below are the shapes of the image at each step after obtaining an image from the resnet-152 model, where the input for this example operation was `(1, 3, 224, 224)` where the no of batches is 1, no of channels is `3` and the shape of the image is `224x224`\n",
    "<br>\n",
    "`img_enc.forward(img)`\n",
    "```\n",
    "Model output torch.Size([1, 2048, 7, 7])\n",
    "Pooling output torch.Size([1, 2048, 7, 1])\n",
    "Flattening output torch.Size([1, 2048, 7])\n",
    "Transpose output torch.Size([1, 7, 2048])\n",
    "\n",
    "tensor([[[1.0920, 0.5761, 0.6760,  ..., 0.5043, 0.0468, 0.8262],\n",
    "         [1.6031, 0.7189, 1.2634,  ..., 0.7476, 0.2092, 0.3963],\n",
    "         [1.4418, 0.3756, 1.0606,  ..., 0.6728, 0.8360, 0.1597],\n",
    "         ...,\n",
    "         [0.8339, 0.6820, 0.6216,  ..., 0.0877, 0.6460, 0.4525],\n",
    "         [0.1193, 0.1641, 0.5969,  ..., 0.2471, 0.5955, 0.0536],\n",
    "         [0.0970, 0.1573, 1.4045,  ..., 0.0740, 0.2112, 0.4067]]],\n",
    "       grad_fn=<CopyBackwards>)\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kf23freetazm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1052207832081129472"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('clean_datav5.csv')\n",
    "(df['tweet_id'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>missing_text</th>\n",
       "      <th>Text_Only_Informative</th>\n",
       "      <th>Image_Only_Informative</th>\n",
       "      <th>Directed_Hate</th>\n",
       "      <th>Generalized_Hate</th>\n",
       "      <th>Sarcasm</th>\n",
       "      <th>Allegation</th>\n",
       "      <th>Justification</th>\n",
       "      <th>Refutation</th>\n",
       "      <th>Support</th>\n",
       "      <th>Oppose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1052237153789390853</td>\n",
       "      <td>new post domestic violence awareness caught me...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1052207832081129472</td>\n",
       "      <td>domestic violence awareness caught metoo</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1052183746344960000</td>\n",
       "      <td>mother nature metoo</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1052156864840908800</td>\n",
       "      <td>ption no2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1052095305133510656</td>\n",
       "      <td>high time metoo named shamed men medium advert...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7973</th>\n",
       "      <td>7973</td>\n",
       "      <td>7973</td>\n",
       "      <td>7973</td>\n",
       "      <td>1052099226799353856</td>\n",
       "      <td>one priyaramani make billion people metooindia...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7974</th>\n",
       "      <td>7974</td>\n",
       "      <td>7974</td>\n",
       "      <td>7974</td>\n",
       "      <td>1052099000688631809</td>\n",
       "      <td>thought metoo limited woman condeming wake rea...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7975</th>\n",
       "      <td>7975</td>\n",
       "      <td>7975</td>\n",
       "      <td>7975</td>\n",
       "      <td>1052098808178302977</td>\n",
       "      <td>wake metoo movement hairstylist sapna bhavani ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7976</th>\n",
       "      <td>7976</td>\n",
       "      <td>7976</td>\n",
       "      <td>7976</td>\n",
       "      <td>1052098776490340352</td>\n",
       "      <td>metoo icc step sexual harassment</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7977</th>\n",
       "      <td>7977</td>\n",
       "      <td>7977</td>\n",
       "      <td>7977</td>\n",
       "      <td>1052098776490340352</td>\n",
       "      <td>long live metoo metooindia hope culprit punish...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7978 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1             tweet_id  \\\n",
       "0              0             0               0  1052237153789390853   \n",
       "1              1             1               1  1052207832081129472   \n",
       "2              2             2               2  1052183746344960000   \n",
       "3              3             3               3  1052156864840908800   \n",
       "4              4             4               4  1052095305133510656   \n",
       "...          ...           ...             ...                  ...   \n",
       "7973        7973          7973            7973  1052099226799353856   \n",
       "7974        7974          7974            7974  1052099000688631809   \n",
       "7975        7975          7975            7975  1052098808178302977   \n",
       "7976        7976          7976            7976  1052098776490340352   \n",
       "7977        7977          7977            7977  1052098776490340352   \n",
       "\n",
       "                                                   text  missing_text  \\\n",
       "0     new post domestic violence awareness caught me...             0   \n",
       "1             domestic violence awareness caught metoo              0   \n",
       "2                                  mother nature metoo              0   \n",
       "3                                            ption no2              0   \n",
       "4     high time metoo named shamed men medium advert...             0   \n",
       "...                                                 ...           ...   \n",
       "7973  one priyaramani make billion people metooindia...             0   \n",
       "7974  thought metoo limited woman condeming wake rea...             0   \n",
       "7975  wake metoo movement hairstylist sapna bhavani ...             0   \n",
       "7976                  metoo icc step sexual harassment              0   \n",
       "7977  long live metoo metooindia hope culprit punish...             0   \n",
       "\n",
       "      Text_Only_Informative  Image_Only_Informative  Directed_Hate  \\\n",
       "0                       1.0                     1.0            0.0   \n",
       "1                       1.0                     1.0            0.0   \n",
       "2                       0.0                     1.0            0.0   \n",
       "3                       1.0                     0.0            1.0   \n",
       "4                       1.0                     1.0            0.0   \n",
       "...                     ...                     ...            ...   \n",
       "7973                    0.0                     0.0            0.0   \n",
       "7974                    0.0                     0.0            0.0   \n",
       "7975                    0.0                     0.0            0.0   \n",
       "7976                    0.0                     0.0            0.0   \n",
       "7977                    0.0                     0.0            0.0   \n",
       "\n",
       "      Generalized_Hate  Sarcasm  Allegation  Justification  Refutation  \\\n",
       "0                  0.0      0.0         0.0            1.0         0.0   \n",
       "1                  0.0      0.0         0.0            0.0         0.0   \n",
       "2                  0.0      0.0         0.0            0.0         0.0   \n",
       "3                  0.0      0.0         1.0            0.0         0.0   \n",
       "4                  0.0      0.0         1.0            0.0         0.0   \n",
       "...                ...      ...         ...            ...         ...   \n",
       "7973               0.0      0.0         0.0            0.0         0.0   \n",
       "7974               0.0      0.0         0.0            0.0         0.0   \n",
       "7975               0.0      0.0         0.0            0.0         0.0   \n",
       "7976               0.0      0.0         0.0            0.0         0.0   \n",
       "7977               0.0      0.0         0.0            0.0         0.0   \n",
       "\n",
       "      Support  Oppose  \n",
       "0         1.0     0.0  \n",
       "1         1.0     0.0  \n",
       "2         0.0     0.0  \n",
       "3         0.0     1.0  \n",
       "4         1.0     0.0  \n",
       "...       ...     ...  \n",
       "7973      0.0     0.0  \n",
       "7974      0.0     0.0  \n",
       "7975      0.0     0.0  \n",
       "7976      0.0     0.0  \n",
       "7977      0.0     0.0  \n",
       "\n",
       "[7978 rows x 16 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    def __init__(self, emptyInit=False):\n",
    "        if emptyInit:\n",
    "            self.stoi={}#string to index dictionary\n",
    "            self.itos=[]#index to string dictionary\n",
    "            self.vocab_size=0\n",
    "        else:\n",
    "            self.stoi={\n",
    "                w:i\n",
    "                for i, w in enumerate([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
    "            }\n",
    "            self.itos = [w for w in self.stoi]\n",
    "            self.vocab_size = len(self.itos)\n",
    "    \n",
    "    def add(self, words):\n",
    "        counter = len(self.itos)\n",
    "        for w in words:\n",
    "            if w in self.stoi:\n",
    "                continue\n",
    "            self.stoi[w]=counter\n",
    "            counter+=1\n",
    "            self.itos.append(w)\n",
    "        self.vocab_size = len(self.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TextNImageDataset(Dataset):\n",
    "    def __init__(self, data, image_path, label_name, transforms, tokenizer, vocab):\n",
    "        self.data = data\n",
    "        self.image_path = (image_path)\n",
    "        self.label_name = label_name\n",
    "        self.transforms = transforms\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_sent_len = 512 - 7 - 2\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def __getitem__(self,  index):\n",
    "        text = self.data['text'][index]\n",
    "        text = self.tokenizer.tokenize(text)[:self.max_sent_len]\n",
    "        text = torch.LongTensor(\n",
    "            [\n",
    "                self.vocab.stoi[w] if w in self.vocab.stoi else self.vocab.stoi[\"[UNK]\"]\n",
    "                for w in text\n",
    "            ]\n",
    "        )\n",
    "        tweet_id = self.data['tweet_id'][index]\n",
    "        label = torch.LongTensor([self.data[self.label_name][index]])\n",
    "        image = None\n",
    "        try:\n",
    "            image = Image.open(\n",
    "                self.image_path+\"/\"+str(tweet_id)+\".jpg\"\n",
    "            ).convert(\"RGB\")\n",
    "#             print(self.image_path+\"/\"+str(tweet_id)+\".jpg\"+\" opened!\")\n",
    "#             image.show()\n",
    "            image = self.transforms(image)\n",
    "        except:\n",
    "            image = Image.fromarray(128*np.ones((256, 256, 3), dtype=np.uint8))\n",
    "            image = self.transforms(image)\n",
    "            \n",
    "        return text, label, image\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transformations = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(256),\n",
    "#             transforms.Resize((224, 244)),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.46777044, 0.44531429, 0.40661017],\n",
    "                std=[0.12221994, 0.12145835, 0.14380469],\n",
    "            ),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3, '[MASK]': 4}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Vocab()\n",
    "vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(\n",
    "            'bert-base-uncased', do_lower_case=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = TextNImageDataset(df, '/home/soham/Desktop/IEEE-BigMM/Data/train_images', 'Sarcasm', img_transformations, bert_tokenizer, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7978"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-56a737a97fc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-131-ca48d0f9af04>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_sent_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mtweet_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "text, label, img = data1.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.4850, -1.4529, -0.5545,  ..., -1.1000, -1.0679, -1.0679],\n",
       "         [-1.4850, -1.4208, -0.5545,  ..., -1.0679, -1.0679, -1.1000],\n",
       "         [-1.5492, -1.4208, -0.5866,  ..., -1.1000, -1.1000, -1.1320],\n",
       "         ...,\n",
       "         [-1.1000, -1.1000, -1.0679,  ..., -2.6401, -2.6722, -2.6722],\n",
       "         [-1.1320, -1.1320, -1.1000,  ..., -2.6722, -2.6722, -2.6722],\n",
       "         [-1.1320, -1.1320, -1.1000,  ..., -2.6722, -2.6722, -2.6722]],\n",
       "\n",
       "        [[-1.1157, -1.1157, -0.3085,  ..., -1.0834, -1.1157, -1.1157],\n",
       "         [-1.1157, -1.0834, -0.2762,  ..., -1.0511, -1.1157, -1.1480],\n",
       "         [-1.1480, -1.0834, -0.3085,  ..., -1.0834, -1.1480, -1.1803],\n",
       "         ...,\n",
       "         [-0.6637, -0.6314, -0.6314,  ..., -2.3426, -2.3749, -2.3749],\n",
       "         [-0.6637, -0.6637, -0.6637,  ..., -2.3749, -2.3749, -2.3749],\n",
       "         [-0.6637, -0.6637, -0.6960,  ..., -2.3749, -2.3749, -2.3749]],\n",
       "\n",
       "        [[-0.7823, -0.8368, -0.1550,  ..., -0.8095, -0.8368, -0.8368],\n",
       "         [-0.7823, -0.8095, -0.1278,  ..., -0.7823, -0.8368, -0.8641],\n",
       "         [-0.8095, -0.7823, -0.1550,  ..., -0.8095, -0.8641, -0.8913],\n",
       "         ...,\n",
       "         [-0.0187, -0.0187,  0.0086,  ..., -1.4095, -1.4095, -1.4095],\n",
       "         [-0.0460, -0.0460, -0.0187,  ..., -1.4367, -1.4367, -1.4367],\n",
       "         [-0.0460, -0.0460, -0.0460,  ..., -1.4367, -1.4367, -1.4367]]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# below is to view the above image fetched\n",
    "# torchvision.transforms.ToPILImage()(img).show()\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_function_for_dataloader(batch, task_type='singlelabel'):\n",
    "    lengths = [len(row[0]) for row in batch]\n",
    "    batch_size = len(batch)\n",
    "    max_sent_len = max(lengths)\n",
    "    if(max_sent_len>512-7-2):\n",
    "        max_sent_len=512-7-2\n",
    "    text_tensors = torch.zeros(batch_size, max_sent_len).long()\n",
    "    text_attention_mask = torch.zeros(batch_size, max_sent_len).long()\n",
    "    text_segment = torch.zeros(batch_size, max_sent_len).long()\n",
    "    \n",
    "    batch_image_tensors = torch.stack([row[2] for row in batch])\n",
    "    label_tensors = torch.cat([row[1] for row in batch]).long()\n",
    "    if task_type=='multilabel':\n",
    "        label_tensors = torch.stack([row[1] for row in batch])\n",
    "#     note there is a difference between stack and cat, refer link below if needed\n",
    "# https://stackoverflow.com/questions/54307225/whats-the-difference-between-torch-stack-and-torch-cat-functions\n",
    "    \n",
    "    for i, (row, length) in enumerate(zip(batch, lengths)):\n",
    "        text_tokens = row[0]\n",
    "        if(length>512-7-2):\n",
    "            length = 512-7-2\n",
    "        text_tensors[i, :length] = text_tokens\n",
    "        text_segment[i, :length] = 1\n",
    "        text_attention_mask[i, :length]=1\n",
    "    \n",
    "    return text_tensors, label_tensors, text_segment, text_attention_mask, batch_image_tensors\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1loader = torch.utils.data.DataLoader(data1, batch_size=4, shuffle=True, collate_fn=collate_function_for_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(data1loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't assign a str to a torch.LongTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-ebba3a8e6d3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-69-42dd87e5f59e>\u001b[0m in \u001b[0;36mcollate_function_for_dataloader\u001b[0;34m(batch, task_type)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mtext_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mtext_segment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtext_attention_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't assign a str to a torch.LongTensor"
     ]
    }
   ],
   "source": [
    "dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'stoi'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-139-1ae7d280bf41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'stoi'"
     ]
    }
   ],
   "source": [
    "bert_tokenizer.vocab['[UNK]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[unused1]'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.ids_to_tokens[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['written',\n",
       " 'met',\n",
       " '##oo',\n",
       " 'created',\n",
       " 'down',\n",
       " '##play',\n",
       " 'rape',\n",
       " '##son',\n",
       " '##nu',\n",
       " '##ns',\n",
       " 'christian',\n",
       " 'father',\n",
       " 'amp',\n",
       " 'bishop',\n",
       " 'clear',\n",
       " 'tan',\n",
       " '##ush',\n",
       " '##ree',\n",
       " '##du',\n",
       " '##tta',\n",
       " 'converted',\n",
       " '##christ',\n",
       " '##ian',\n",
       " 'rest',\n",
       " 'left',\n",
       " 'reader']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.tokenize('written metoo created downplay rapesonnuns christian father amp bishop clear tanushreedutta convertedchristian rest left reader ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertEncoder(\n",
       "  (layer): ModuleList(\n",
       "    (0): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertEmbeddings(\n",
       "  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (token_type_embeddings): Embedding(2, 768)\n",
       "  (LayerNorm): BertLayerNorm()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertPooler(\n",
       "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (activation): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert.pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEmbeddingsForBert(nn.Module):\n",
    "    def __init__(self, embeddings, vocabObject):\n",
    "        self.vocab = vocabObject\n",
    "#       the embeddins received as input are the \n",
    "#       all the embeddings provided by the bert model from pytorch\n",
    "        self.img_embeddings = nn.Linear(2048, 768)\n",
    "#       above is linear layer is used to convert the flattened images \n",
    "#       logits obtained after pooling from Image encoder which have 2048\n",
    "#       dimensions to a 768 dimensions which is the size of bert's hidden layer\n",
    "        \n",
    "        self.position_embeddings = embeddings.position_embeddings\n",
    "        self.token_type_embeddings = embeddings.token_type_embdeddings\n",
    "        self.word_embeddings = embeddings.word_embeddings\n",
    "        self.LayerNorm = embeddings.LayerNorm\n",
    "        self.dropout = embeddings.dropout\n",
    "        \n",
    "    def forward(self, batch_input_imgs, token_type_ids):\n",
    "        batch_size = batch_input_imgs.size(0)\n",
    "        seq_length = 7 + 2\n",
    "#         since we are assuming that from each image we will obtain\n",
    "#         7 image embeddings of 768 dimensions each\n",
    "        \n",
    "        cls_id = torch.LongTensor([self.vocab.stoi[\"[CLS]\"]])\n",
    "        if torch.cuda.is_available():\n",
    "            cls_id = clis_id.cuda()\n",
    "        cls_id = cls_id.unsqueeze(0).expand(batch_size, 1)\n",
    "        cls_token_embeddings = self.word_embeddings(cls_id)\n",
    "        \n",
    "        sep_id = torch.LongTensor([self.vocab.stoi[\"[SEP]\"]])\n",
    "        if torch.cuda.is_available():\n",
    "            sep_id = sep_id.cuda()\n",
    "        sep_id = sep_id.unsqueeze(0).expand(batch_size, 1)\n",
    "        sep_token_embeddings = self.word_embeddings(sep_id)\n",
    "        \n",
    "        batch_image_embeddings_768 = self.image_embeddings(batch_input_imgs)\n",
    "        \n",
    "        token_embeddings = torch.cat(\n",
    "        [cls_token_embeddings, batch_image_embeddings_768, sep_token_embeddings], dim=1)\n",
    "        \n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long)\n",
    "        if torch.cuda.is_available():\n",
    "            position_ids = position_ids.cuda()\n",
    "        position_ids = position_ids.unsqueeze(0).expand(batch_size, seq_length)\n",
    "        \n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        \n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "        \n",
    "        embeddings = token_embeddings+position_embeddings+token_type_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalBertEncoder(nn.Module):\n",
    "    def __init__(self, no_of_classes, tokenizer):\n",
    "        super(MultimodalBertEncoder, self).__init__()\n",
    "        bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.tokenizer = tokenizer\n",
    "        self.embeddings = bert.embeddings\n",
    "        self.vocab=Vocab()\n",
    "        self.image_embeddings = ImageEmbeddingsForBert(self.embeddings, self.vocab)\n",
    "        self.image_encoder = ImageEncoder()\n",
    "        self.encoder = bert.encoder\n",
    "        self.pooler = bert.pooler\n",
    "        self.clf = nn.Linear(768, no_of_classes)\n",
    "        \n",
    "    def forward(self, input_text, text_attention_mask, text_segment, input_image):\n",
    "        batch_size = input_text.size(0)\n",
    "# input text is a tensor of encoded texts!\n",
    "        temp = torch.ones(batch_size, 7+2).long()\n",
    "        if torch.cuda.is_available():\n",
    "            temp = temp.cuda()\n",
    "        attention_mask = torch.cat(\n",
    "            [\n",
    "                temp, text_attention_mask\n",
    "            ],\n",
    "            dim=1\n",
    "        )\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "        extended_attention_mask = extended_attention_mask.to(\n",
    "            dtype=next(self.parameters()).dtype\n",
    "        )\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask)*-10000.0\n",
    "        \n",
    "        image_token_type_ids = torch.LongTensor(batch_size, 7+2).fill_(0)\n",
    "        if(torch.cuda.is_available()):\n",
    "            image_token_type_ids= image_token_type_ids.cuda()\n",
    "        \n",
    "        image = self.image_encoder(input_image)\n",
    "#         above image returned is of the formc nC x nH x nW and is a tensor\n",
    "        image_embedding_out = self.image_embeddings(image, image_token_type_ids)\n",
    "        print('Image embeddings: ', image_embedding_out.size())\n",
    "        \n",
    "        text_embedding_out = self.embeddings(input_text, segment)\n",
    "        print('Text embeddings: ', text_embedding_out.size(), text_embedding_out)\n",
    "        \n",
    "        \n",
    "        encoder_input = torch.cat([image_embedding_out, text_embedding_out], dim=1)\n",
    "#         the encoder input is of the form CLS (7 image embeddings) SEP text_embeddings\n",
    "    \n",
    "        encoded_layers = self.encoder(encoder_input, extended_attention_mask, output_all_encoded_layers=False)\n",
    "        print('encoded layers', encoded_layers)\n",
    "        return self.pooler(encoder_layers[-1])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalBertClf(nn.Module):\n",
    "    def __init__(self, no_of_classes):\n",
    "        super(MultiModalBertClf, self).__init__()\n",
    "        self.no_of_classes = no_of_classes\n",
    "        self.enc = MultiModalBertEncoder(self.no_of_classes)\n",
    "        self.clf = nn.Linear(768, self.no_of_classes)\n",
    "    \n",
    "    def forward(self, text, text_attention_mask, text_segment, image):\n",
    "        x = self.enc(txt, mask, segment, img)\n",
    "        x = self.clf(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled9.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
