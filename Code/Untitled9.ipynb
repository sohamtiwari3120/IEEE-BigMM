{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0brZnNWfhBiH"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pytorch_pretrained_bert.modeling import BertModel\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VtJttWabwJR7"
   },
   "source": [
    "# **Image Handling Resnet-152**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i1MKtV0mkk46"
   },
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        model = torchvision.models.resnet152(pretrained=True)\n",
    "        modules = list(model.children())[:-2]\n",
    "        # we are removing the last adaptive average pooling layer and the \n",
    "        # the classification layer\n",
    "        self.model = nn.Sequential(*modules)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = (self.model(x))\n",
    "        # print('Model output', out.size())\n",
    "\n",
    "        out = nn.AdaptiveAvgPool2d((7, 1))(out)#specifying the H and W of the image\n",
    "        # to be obtained after pooling\n",
    "        # print('Pooling output', out.size())\n",
    "\n",
    "        out = torch.flatten(out, start_dim=2)\n",
    "        # print('Flattening output', out.size())\n",
    "\n",
    "        out = out.transpose(1, 2).contiguous()\n",
    "        # print('Transpose output', out.size())\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B7nRPX6mlhnA"
   },
   "source": [
    "\n",
    "### Important Note:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6k-f07rTwoWn"
   },
   "source": [
    "Understanding the process of obtaining N(in this case 7) 2048 dimensional image embeddings\n",
    "<br>\n",
    "<br>\n",
    "Below is the example of a sample image\n",
    "```\n",
    "img_enc = ImageEncoder()\n",
    "img = torch.randn(1, 3, 224, 224)\n",
    "img\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "tensor([[[[ 0.4623, -0.0570,  0.1685,  ..., -0.6377, -0.4702,  0.8996],\n",
    "          [ 0.5874,  0.1590, -0.2373,  ..., -1.7897, -0.3391, -1.0945],\n",
    "          [ 0.6259,  1.3741,  0.6457,  ..., -0.3259,  0.2340,  0.5563],\n",
    "          ...,\n",
    "          [-0.3431,  0.8013, -1.1648,  ...,  0.3589, -1.0933,  0.0880],\n",
    "          [ 0.3228, -2.2501,  1.8554,  ...,  0.6990,  1.2223, -0.6696],\n",
    "          [ 0.0949,  0.3022, -1.7768,  ...,  0.5936,  1.3039,  1.4402]],\n",
    "\n",
    "         [[-0.7338,  0.3525, -0.0956,  ..., -0.5781, -0.8532, -0.9768],\n",
    "          [ 0.3267, -0.4692,  0.2099,  ...,  0.8854, -0.0515, -0.9874],\n",
    "          [ 2.0738, -0.5577,  0.3773,  ...,  0.9743, -2.0519,  0.0128],\n",
    "          ...,\n",
    "          [-0.1382, -0.8803,  0.6664,  ..., -0.3854, -1.2113,  1.0680],\n",
    "          [-0.8094,  0.6352, -0.1113,  ..., -2.2602,  0.3099,  0.2487],\n",
    "          [-0.3672,  1.2410,  0.0260,  ..., -0.0627,  0.2084, -0.2197]],\n",
    "\n",
    "         [[ 0.6515, -0.2968, -0.1592,  ..., -0.0610,  0.3312, -0.9807],\n",
    "          [-1.9452, -1.1792, -0.3001,  ...,  0.5704,  1.4844, -1.4242],\n",
    "          [ 0.1115, -0.1929,  0.0363,  ...,  0.8737,  0.2437,  0.4418],\n",
    "          ...,\n",
    "          [ 1.6531,  0.0160, -0.6031,  ...,  0.8056, -0.5860, -0.2903],\n",
    "          [-0.1911, -1.4188, -0.2629,  ..., -1.3827, -0.7149, -2.4575],\n",
    "          [-1.5174, -1.5290, -0.3920,  ...,  1.0713,  0.4248, -0.2714]]]])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E9uWEL1GugDz"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "```\n",
    "img.size()\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "torch.Size([1, 3, 224, 224])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hB-g3adLnBfd"
   },
   "source": [
    "**Note:**<br>\n",
    "Below are the shapes of the image at each step after obtaining an image from the resnet-152 model, where the input for this example operation was `(1, 3, 224, 224)` where the no of batches is 1, no of channels is `3` and the shape of the image is `224x224`\n",
    "<br>\n",
    "`img_enc.forward(img)`\n",
    "```\n",
    "Model output torch.Size([1, 2048, 7, 7])\n",
    "Pooling output torch.Size([1, 2048, 7, 1])\n",
    "Flattening output torch.Size([1, 2048, 7])\n",
    "Transpose output torch.Size([1, 7, 2048])\n",
    "\n",
    "tensor([[[1.0920, 0.5761, 0.6760,  ..., 0.5043, 0.0468, 0.8262],\n",
    "         [1.6031, 0.7189, 1.2634,  ..., 0.7476, 0.2092, 0.3963],\n",
    "         [1.4418, 0.3756, 1.0606,  ..., 0.6728, 0.8360, 0.1597],\n",
    "         ...,\n",
    "         [0.8339, 0.6820, 0.6216,  ..., 0.0877, 0.6460, 0.4525],\n",
    "         [0.1193, 0.1641, 0.5969,  ..., 0.2471, 0.5955, 0.0536],\n",
    "         [0.0970, 0.1573, 1.4045,  ..., 0.0740, 0.2112, 0.4067]]],\n",
    "       grad_fn=<CopyBackwards>)\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kf23freetazm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1052207832081129472"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('clean_datav5.csv')\n",
    "(df['tweet_id'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>missing_text</th>\n",
       "      <th>Text_Only_Informative</th>\n",
       "      <th>Image_Only_Informative</th>\n",
       "      <th>Directed_Hate</th>\n",
       "      <th>Generalized_Hate</th>\n",
       "      <th>Sarcasm</th>\n",
       "      <th>Allegation</th>\n",
       "      <th>Justification</th>\n",
       "      <th>Refutation</th>\n",
       "      <th>Support</th>\n",
       "      <th>Oppose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1052237153789390853</td>\n",
       "      <td>new post domestic violence awareness caught me...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1052207832081129472</td>\n",
       "      <td>domestic violence awareness caught metoo</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1052183746344960000</td>\n",
       "      <td>mother nature metoo</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1052156864840908800</td>\n",
       "      <td>ption no2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1052095305133510656</td>\n",
       "      <td>high time metoo named shamed men medium advert...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7973</th>\n",
       "      <td>7973</td>\n",
       "      <td>7973</td>\n",
       "      <td>7973</td>\n",
       "      <td>1052099226799353856</td>\n",
       "      <td>one priyaramani make billion people metooindia...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7974</th>\n",
       "      <td>7974</td>\n",
       "      <td>7974</td>\n",
       "      <td>7974</td>\n",
       "      <td>1052099000688631809</td>\n",
       "      <td>thought metoo limited woman condeming wake rea...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7975</th>\n",
       "      <td>7975</td>\n",
       "      <td>7975</td>\n",
       "      <td>7975</td>\n",
       "      <td>1052098808178302977</td>\n",
       "      <td>wake metoo movement hairstylist sapna bhavani ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7976</th>\n",
       "      <td>7976</td>\n",
       "      <td>7976</td>\n",
       "      <td>7976</td>\n",
       "      <td>1052098776490340352</td>\n",
       "      <td>metoo icc step sexual harassment</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7977</th>\n",
       "      <td>7977</td>\n",
       "      <td>7977</td>\n",
       "      <td>7977</td>\n",
       "      <td>1052098776490340352</td>\n",
       "      <td>long live metoo metooindia hope culprit punish...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7978 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1             tweet_id  \\\n",
       "0              0             0               0  1052237153789390853   \n",
       "1              1             1               1  1052207832081129472   \n",
       "2              2             2               2  1052183746344960000   \n",
       "3              3             3               3  1052156864840908800   \n",
       "4              4             4               4  1052095305133510656   \n",
       "...          ...           ...             ...                  ...   \n",
       "7973        7973          7973            7973  1052099226799353856   \n",
       "7974        7974          7974            7974  1052099000688631809   \n",
       "7975        7975          7975            7975  1052098808178302977   \n",
       "7976        7976          7976            7976  1052098776490340352   \n",
       "7977        7977          7977            7977  1052098776490340352   \n",
       "\n",
       "                                                   text  missing_text  \\\n",
       "0     new post domestic violence awareness caught me...             0   \n",
       "1             domestic violence awareness caught metoo              0   \n",
       "2                                  mother nature metoo              0   \n",
       "3                                            ption no2              0   \n",
       "4     high time metoo named shamed men medium advert...             0   \n",
       "...                                                 ...           ...   \n",
       "7973  one priyaramani make billion people metooindia...             0   \n",
       "7974  thought metoo limited woman condeming wake rea...             0   \n",
       "7975  wake metoo movement hairstylist sapna bhavani ...             0   \n",
       "7976                  metoo icc step sexual harassment              0   \n",
       "7977  long live metoo metooindia hope culprit punish...             0   \n",
       "\n",
       "      Text_Only_Informative  Image_Only_Informative  Directed_Hate  \\\n",
       "0                       1.0                     1.0            0.0   \n",
       "1                       1.0                     1.0            0.0   \n",
       "2                       0.0                     1.0            0.0   \n",
       "3                       1.0                     0.0            1.0   \n",
       "4                       1.0                     1.0            0.0   \n",
       "...                     ...                     ...            ...   \n",
       "7973                    0.0                     0.0            0.0   \n",
       "7974                    0.0                     0.0            0.0   \n",
       "7975                    0.0                     0.0            0.0   \n",
       "7976                    0.0                     0.0            0.0   \n",
       "7977                    0.0                     0.0            0.0   \n",
       "\n",
       "      Generalized_Hate  Sarcasm  Allegation  Justification  Refutation  \\\n",
       "0                  0.0      0.0         0.0            1.0         0.0   \n",
       "1                  0.0      0.0         0.0            0.0         0.0   \n",
       "2                  0.0      0.0         0.0            0.0         0.0   \n",
       "3                  0.0      0.0         1.0            0.0         0.0   \n",
       "4                  0.0      0.0         1.0            0.0         0.0   \n",
       "...                ...      ...         ...            ...         ...   \n",
       "7973               0.0      0.0         0.0            0.0         0.0   \n",
       "7974               0.0      0.0         0.0            0.0         0.0   \n",
       "7975               0.0      0.0         0.0            0.0         0.0   \n",
       "7976               0.0      0.0         0.0            0.0         0.0   \n",
       "7977               0.0      0.0         0.0            0.0         0.0   \n",
       "\n",
       "      Support  Oppose  \n",
       "0         1.0     0.0  \n",
       "1         1.0     0.0  \n",
       "2         0.0     0.0  \n",
       "3         0.0     1.0  \n",
       "4         1.0     0.0  \n",
       "...       ...     ...  \n",
       "7973      0.0     0.0  \n",
       "7974      0.0     0.0  \n",
       "7975      0.0     0.0  \n",
       "7976      0.0     0.0  \n",
       "7977      0.0     0.0  \n",
       "\n",
       "[7978 rows x 16 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    def __init__(self, emptyInit=False):\n",
    "        if emptyInit:\n",
    "            self.stoi={}#string to index dictionary\n",
    "            self.itos=[]#index to string dictionary\n",
    "            self.vocab_size=0\n",
    "        else:\n",
    "            self.stoi={\n",
    "                w:i\n",
    "                for i, w in enumerate([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
    "            }\n",
    "            self.itos = [w for w in self.stoi]\n",
    "            self.vocab_size = len(self.itos)\n",
    "    \n",
    "    def add(self, words):\n",
    "        counter = len(self.itos)\n",
    "        for w in words:\n",
    "            if w in self.stoi:\n",
    "                continue\n",
    "            self.stoi[w]=counter\n",
    "            counter+=1\n",
    "            self.itos.append(w)\n",
    "        self.vocab_size = len(self.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TextNImageDataset(Dataset):\n",
    "    def __init__(self, data, image_path, label_name, transforms, tokenizer, vocab):\n",
    "        self.data = data\n",
    "        self.image_path = (image_path)\n",
    "        self.label_name = label_name\n",
    "        self.transforms = transforms\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_sent_len = 512 - 7 - 2\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def __getitem__(self,  index):\n",
    "        text = self.data['text'][index]\n",
    "        text = self.tokenizer.tokenize(text)[:self.max_sent_len]\n",
    "        text = torch.LongTensor(\n",
    "            [\n",
    "                self.tokenizer.vocab[w] if w in self.tokenizer.vocab else self.vocab.stoi[\"[UNK]\"]\n",
    "                for w in text\n",
    "            ]\n",
    "        )\n",
    "        tweet_id = self.data['tweet_id'][index]\n",
    "        label = torch.LongTensor([self.data[self.label_name][index]])\n",
    "        image = None\n",
    "        try:\n",
    "            image = Image.open(\n",
    "                self.image_path+\"/\"+str(tweet_id)+\".jpg\"\n",
    "            ).convert(\"RGB\")\n",
    "#             print(self.image_path+\"/\"+str(tweet_id)+\".jpg\"+\" opened!\")\n",
    "#             image.show()\n",
    "            image = self.transforms(image)\n",
    "        except:\n",
    "            image = Image.fromarray(128*np.ones((256, 256, 3), dtype=np.uint8))\n",
    "            image = self.transforms(image)\n",
    "            \n",
    "        return text, label, image\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transformations = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(256),\n",
    "#             transforms.Resize((224, 244)),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.46777044, 0.44531429, 0.40661017],\n",
    "                std=[0.12221994, 0.12145835, 0.14380469],\n",
    "            ),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3, '[MASK]': 4}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Vocab()\n",
    "vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(\n",
    "            'bert-base-uncased', do_lower_case=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = TextNImageDataset(df, '/home/soham/Desktop/IEEE-BigMM/Data/train_images', 'Sarcasm', img_transformations, bert_tokenizer, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7978"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, label, img = data1.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.4850, -1.4529, -0.5545,  ..., -1.1000, -1.0679, -1.0679],\n",
       "         [-1.4850, -1.4208, -0.5545,  ..., -1.0679, -1.0679, -1.1000],\n",
       "         [-1.5492, -1.4208, -0.5866,  ..., -1.1000, -1.1000, -1.1320],\n",
       "         ...,\n",
       "         [-1.1000, -1.1000, -1.0679,  ..., -2.6401, -2.6722, -2.6722],\n",
       "         [-1.1320, -1.1320, -1.1000,  ..., -2.6722, -2.6722, -2.6722],\n",
       "         [-1.1320, -1.1320, -1.1000,  ..., -2.6722, -2.6722, -2.6722]],\n",
       "\n",
       "        [[-1.1157, -1.1157, -0.3085,  ..., -1.0834, -1.1157, -1.1157],\n",
       "         [-1.1157, -1.0834, -0.2762,  ..., -1.0511, -1.1157, -1.1480],\n",
       "         [-1.1480, -1.0834, -0.3085,  ..., -1.0834, -1.1480, -1.1803],\n",
       "         ...,\n",
       "         [-0.6637, -0.6314, -0.6314,  ..., -2.3426, -2.3749, -2.3749],\n",
       "         [-0.6637, -0.6637, -0.6637,  ..., -2.3749, -2.3749, -2.3749],\n",
       "         [-0.6637, -0.6637, -0.6960,  ..., -2.3749, -2.3749, -2.3749]],\n",
       "\n",
       "        [[-0.7823, -0.8368, -0.1550,  ..., -0.8095, -0.8368, -0.8368],\n",
       "         [-0.7823, -0.8095, -0.1278,  ..., -0.7823, -0.8368, -0.8641],\n",
       "         [-0.8095, -0.7823, -0.1550,  ..., -0.8095, -0.8641, -0.8913],\n",
       "         ...,\n",
       "         [-0.0187, -0.0187,  0.0086,  ..., -1.4095, -1.4095, -1.4095],\n",
       "         [-0.0460, -0.0460, -0.0187,  ..., -1.4367, -1.4367, -1.4367],\n",
       "         [-0.0460, -0.0460, -0.0460,  ..., -1.4367, -1.4367, -1.4367]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# below is to view the above image fetched\n",
    "# torchvision.transforms.ToPILImage()(img).show()\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2047, 2695, 4968, 4808, 7073, 3236, 2777, 9541, 2047, 2259, 2051, 2405,\n",
       "        2453, 2131, 4906])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_function_for_dataloader(batch, task_type='singlelabel'):\n",
    "    lengths = [len(row[0]) for row in batch]\n",
    "    batch_size = len(batch)\n",
    "    max_sent_len = max(lengths)\n",
    "    if(max_sent_len>512-7-2):\n",
    "        max_sent_len=512-7-2\n",
    "    text_tensors = torch.zeros(batch_size, max_sent_len).long()\n",
    "    text_attention_mask = torch.zeros(batch_size, max_sent_len).long()\n",
    "    text_segment = torch.zeros(batch_size, max_sent_len).long()\n",
    "    \n",
    "    batch_image_tensors = torch.stack([row[2] for row in batch])\n",
    "    label_tensors = torch.cat([row[1] for row in batch]).long()\n",
    "    if task_type=='multilabel':\n",
    "        label_tensors = torch.stack([row[1] for row in batch])\n",
    "#     note there is a difference between stack and cat, refer link below if needed\n",
    "# https://stackoverflow.com/questions/54307225/whats-the-difference-between-torch-stack-and-torch-cat-functions\n",
    "    \n",
    "    for i, (row, length) in enumerate(zip(batch, lengths)):\n",
    "        text_tokens = row[0]\n",
    "        if(length>512-7-2):\n",
    "            length = 512-7-2\n",
    "        text_tensors[i, :length] = text_tokens\n",
    "        text_segment[i, :length] = 1\n",
    "        text_attention_mask[i, :length]=1\n",
    "    \n",
    "    return text_tensors, label_tensors, text_segment, text_attention_mask, batch_image_tensors\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1loader = torch.utils.data.DataLoader(data1, batch_size=4, shuffle=True, collate_fn=collate_function_for_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(data1loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tensors, label_tensors, text_segment, text_attention_mask, batch_image_tensors=dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3342,  2106, 29337, 19895,  3049, 21317,  3647,  2686,  5095, 26047,\n",
       "          9619, 13893,  2051,  2777,  9541,  3693,  1057, 20228,  8718,  9032,\n",
       "          9266,  3215,     0,     0,     0,     0,     0],\n",
       "        [18520,  7207,  2360,  3129, 26960,  2099,  4906,  2777,  9541,  7984,\n",
       "          8529, 18520, 20464, 27028,  2785,  7984,  3331,  4906,  7984,  9616,\n",
       "          2111, 11693, 11234, 25506,  4861,  7564,  7984],\n",
       "        [ 3191,  3104,  2466,  2113,  2777,  9541,  2929,  3068,  2191,  2488,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [13866,  3258,  2053,  2475,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.vocab['[UNK]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[unused1]'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.ids_to_tokens[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['written',\n",
       " 'met',\n",
       " '##oo',\n",
       " 'created',\n",
       " 'down',\n",
       " '##play',\n",
       " 'rape',\n",
       " '##son',\n",
       " '##nu',\n",
       " '##ns',\n",
       " 'christian',\n",
       " 'father',\n",
       " 'amp',\n",
       " 'bishop',\n",
       " 'clear',\n",
       " 'tan',\n",
       " '##ush',\n",
       " '##ree',\n",
       " '##du',\n",
       " '##tta',\n",
       " 'converted',\n",
       " '##christ',\n",
       " '##ian',\n",
       " 'rest',\n",
       " 'left',\n",
       " 'reader']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.tokenize('written metoo created downplay rapesonnuns christian father amp bishop clear tanushreedutta convertedchristian rest left reader ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertEncoder(\n",
       "  (layer): ModuleList(\n",
       "    (0): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertEmbeddings(\n",
       "  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (token_type_embeddings): Embedding(2, 768)\n",
       "  (LayerNorm): BertLayerNorm()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertPooler(\n",
       "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (activation): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert.pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEmbeddingsForBert(nn.Module):\n",
    "    def __init__(self, embeddings, vocabObject):\n",
    "        super(ImageEmbeddingsForBert, self).__init__()\n",
    "        self.vocab = vocabObject\n",
    "#       the embeddins received as input are the \n",
    "#       all the embeddings provided by the bert model from pytorch\n",
    "        self.img_embeddings = nn.Linear(2048, 768)\n",
    "#       above is linear layer is used to convert the flattened images \n",
    "#       logits obtained after pooling from Image encoder which have 2048\n",
    "#       dimensions to a 768 dimensions which is the size of bert's hidden layer\n",
    "        \n",
    "        self.position_embeddings = embeddings.position_embeddings\n",
    "        self.token_type_embeddings = embeddings.token_type_embeddings\n",
    "        self.word_embeddings = embeddings.word_embeddings\n",
    "        self.LayerNorm = embeddings.LayerNorm\n",
    "        self.dropout = embeddings.dropout\n",
    "        \n",
    "    def forward(self, batch_input_imgs, token_type_ids):\n",
    "        batch_size = batch_input_imgs.size(0)\n",
    "        seq_length = 7 + 2\n",
    "#         since we are assuming that from each image we will obtain\n",
    "#         7 image embeddings of 768 dimensions each\n",
    "        \n",
    "        cls_id = torch.LongTensor([self.vocab.stoi[\"[CLS]\"]])\n",
    "        if torch.cuda.is_available():\n",
    "            cls_id = clis_id.cuda()\n",
    "        cls_id = cls_id.unsqueeze(0).expand(batch_size, 1)\n",
    "        cls_token_embeddings = self.word_embeddings(cls_id)\n",
    "        \n",
    "        sep_id = torch.LongTensor([self.vocab.stoi[\"[SEP]\"]])\n",
    "        if torch.cuda.is_available():\n",
    "            sep_id = sep_id.cuda()\n",
    "        sep_id = sep_id.unsqueeze(0).expand(batch_size, 1)\n",
    "        sep_token_embeddings = self.word_embeddings(sep_id)\n",
    "        \n",
    "        batch_image_embeddings_768 = self.img_embeddings(batch_input_imgs)\n",
    "        \n",
    "        token_embeddings = torch.cat(\n",
    "        [cls_token_embeddings, batch_image_embeddings_768, sep_token_embeddings], dim=1)\n",
    "        \n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long)\n",
    "        if torch.cuda.is_available():\n",
    "            position_ids = position_ids.cuda()\n",
    "        position_ids = position_ids.unsqueeze(0).expand(batch_size, seq_length)\n",
    "        \n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        \n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "        \n",
    "        embeddings = token_embeddings+position_embeddings+token_type_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalBertEncoder(nn.Module):\n",
    "    def __init__(self, no_of_classes, tokenizer):\n",
    "        super(MultiModalBertEncoder, self).__init__()\n",
    "        bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.tokenizer = tokenizer\n",
    "        self.embeddings = bert.embeddings\n",
    "        self.vocab=Vocab()\n",
    "        self.image_embeddings = ImageEmbeddingsForBert(self.embeddings, self.vocab)\n",
    "        self.image_encoder = ImageEncoder()\n",
    "        self.encoder = bert.encoder\n",
    "        self.pooler = bert.pooler\n",
    "        self.clf = nn.Linear(768, no_of_classes)\n",
    "        \n",
    "    def forward(self, input_text, text_attention_mask, text_segment, input_image):\n",
    "        batch_size = input_text.size(0)\n",
    "# input text is a tensor of encoded texts!\n",
    "        temp = torch.ones(batch_size, 7+2).long()\n",
    "        if torch.cuda.is_available():\n",
    "            temp = temp.cuda()\n",
    "        attention_mask = torch.cat(\n",
    "            [\n",
    "                temp, text_attention_mask\n",
    "            ],\n",
    "            dim=1\n",
    "        )\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "        extended_attention_mask = extended_attention_mask.to(\n",
    "            dtype=next(self.parameters()).dtype\n",
    "        )\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask)*-10000.0\n",
    "        \n",
    "        image_token_type_ids = torch.LongTensor(batch_size, 7+2).fill_(0)\n",
    "        if(torch.cuda.is_available()):\n",
    "            image_token_type_ids= image_token_type_ids.cuda()\n",
    "        \n",
    "        image = self.image_encoder(input_image)\n",
    "#         above image returned is of the formc nC x nH x nW and is a tensor\n",
    "        image_embedding_out = self.image_embeddings(image, image_token_type_ids)\n",
    "        print('Image embeddings: ', image_embedding_out.size())\n",
    "        \n",
    "        text_embedding_out = self.embeddings(input_text, segment)\n",
    "        print('Text embeddings: ', text_embedding_out.size(), text_embedding_out)\n",
    "        \n",
    "        \n",
    "        encoder_input = torch.cat([image_embedding_out, text_embedding_out], dim=1)\n",
    "#         the encoder input is of the form CLS (7 image embeddings) SEP text_embeddings\n",
    "    \n",
    "        encoded_layers = self.encoder(encoder_input, extended_attention_mask, output_all_encoded_layers=False)\n",
    "        print('encoded layers', encoded_layers)\n",
    "        return self.pooler(encoder_layers[-1])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalBertClf(nn.Module):\n",
    "    def __init__(self, no_of_classes, tokenizer):\n",
    "        super(MultiModalBertClf, self).__init__()\n",
    "        self.no_of_classes = no_of_classes\n",
    "        self.enc = MultiModalBertEncoder(self.no_of_classes, tokenizer)\n",
    "        self.clf = nn.Linear(768, self.no_of_classes)\n",
    "    \n",
    "    def forward(self, text, text_attention_mask, text_segment, image):\n",
    "        x = self.enc(text, text_attention_mask, text_segment, image)\n",
    "        x = self.clf(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiModalBertClf(1, bert_tokenizer)(text_tensors, text_attention_mask, text_segment, batch_image_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled9.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
